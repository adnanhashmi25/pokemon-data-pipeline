# PokÃ©mon Data Pipeline  

ğŸš€ **Overview:**  
This project is an **Apache Airflow ETL pipeline** that extracts PokÃ©mon data from an API, processes it, and stores it in CSV format. The pipeline is designed for **scalability** and can be run **locally or inside a Docker container** using Airflow.  

ğŸ”’ **Note:**  
- **API Source:** PokÃ©mon data is fetched from the [PokeAPI](https://pokeapi.co/).  
- **Storage:** The processed data is saved as a CSV file.  
- **Deployment:** The pipeline can be run **locally** or via **Docker + Airflow**.  

---

## ğŸ“Œ Installation & Setup  

### 1ï¸âƒ£ Prerequisites  
Ensure you have the following installed:  
- **Python 3.11**  
- **Apache Airflow** (for local execution)  
- **Docker & Docker Compose** (for containerized execution)  

### **ğŸ’» Running the Pipeline Locally**  
#### **2ï¸âƒ£ Clone the Repository**  
git clone https://github.com/adnanhashmi25/pokemon-data-pipeline.git
cd pokemon-data-pipeline

#### 3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

#### 4ï¸âƒ£ Start Airflow & Trigger the DAG
airflow standalone
Open Airflow UI at http://localhost:8080
Enable & trigger the PokÃ©mon Data Pipeline DAG
### ğŸ³ Running the Pipeline with Docker
#### 2ï¸âƒ£ Set Up Docker & Airflow
cd docker
docker-compose up -d

#### 3ï¸âƒ£ Access Airflow UI
Open: http://localhost:8080
Login: airflow / airflow

#### 4ï¸âƒ£ Run the Pipeline
Go to DAGs section
Enable & trigger pokemon_data_pipeline

### ğŸ“ˆ Features
âœ” Extracts PokÃ©mon data via API
âœ” Processes & stores data in CSV format
âœ” Designed for local & Docker-based execution
âœ” Automated scheduling via Apache Airflow

### ğŸ“œ Detailed Documentation
For a full project breakdown, visit the docs folder.
 - Project Summary
